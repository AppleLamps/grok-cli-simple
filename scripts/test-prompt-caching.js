#!/usr/bin/env node

/**
 * Test script to verify prompt caching implementation
 * Tests both automatic and manual caching configurations
 */

const { LampCode } = require('../lib/lampcode');

console.log('üß™ Testing Prompt Caching Implementation\n');
console.log('='.repeat(60));

// Test 1: Grok (Automatic Caching)
console.log('\nüìù Test 1: Grok Code Fast 1 (Automatic Caching)');
console.log('-'.repeat(60));

const grokLamp = new LampCode();
const grokConfig = grokLamp.getModelConfig();

console.log('Model:', grokLamp.model);
console.log('Max Input Tokens:', grokConfig.maxInputTokens.toLocaleString());
console.log('Supports Caching:', grokLamp.supportsPromptCaching());
console.log('Auto Caching:', grokConfig.autoCaching);
console.log('Requires Manual Cache Control:', grokLamp.requiresManualCaching());

if (grokConfig.maxInputTokens === 256000 && 
    grokConfig.autoCaching === true && 
    grokLamp.requiresManualCaching() === false) {
  console.log('‚úÖ PASS: Grok configured correctly');
} else {
  console.log('‚ùå FAIL: Grok configuration incorrect');
  process.exit(1);
}

// Test 2: Claude (Manual Caching)
console.log('\nüìù Test 2: Anthropic Claude (Manual Caching)');
console.log('-'.repeat(60));

process.env.OPENROUTER_MODEL = 'anthropic/claude-3.5-sonnet';
const claudeLamp = new LampCode();
const claudeConfig = claudeLamp.getModelConfig();

console.log('Model:', claudeLamp.model);
console.log('Max Input Tokens:', claudeConfig.maxInputTokens.toLocaleString());
console.log('Supports Caching:', claudeLamp.supportsPromptCaching());
console.log('Auto Caching:', claudeConfig.autoCaching);
console.log('Requires Manual Cache Control:', claudeLamp.requiresManualCaching());
console.log('Max Cache Breakpoints:', claudeConfig.cacheBreakpoints);

if (claudeConfig.maxInputTokens === 180000 && 
    claudeConfig.autoCaching === false && 
    claudeLamp.requiresManualCaching() === true &&
    claudeConfig.cacheBreakpoints === 4) {
  console.log('‚úÖ PASS: Claude configured correctly');
} else {
  console.log('‚ùå FAIL: Claude configuration incorrect');
  process.exit(1);
}

// Test 3: OpenAI (Automatic Caching with Minimum)
console.log('\nüìù Test 3: OpenAI GPT-4 (Automatic Caching)');
console.log('-'.repeat(60));

process.env.OPENROUTER_MODEL = 'openai/gpt-4';
const openaiLamp = new LampCode();
const openaiConfig = openaiLamp.getModelConfig();

console.log('Model:', openaiLamp.model);
console.log('Max Input Tokens:', openaiConfig.maxInputTokens.toLocaleString());
console.log('Supports Caching:', openaiLamp.supportsPromptCaching());
console.log('Auto Caching:', openaiConfig.autoCaching);
console.log('Min Cache Tokens:', openaiConfig.minCacheTokens);
console.log('Requires Manual Cache Control:', openaiLamp.requiresManualCaching());

if (openaiConfig.maxInputTokens === 120000 && 
    openaiConfig.autoCaching === true && 
    openaiLamp.requiresManualCaching() === false &&
    openaiConfig.minCacheTokens === 1024) {
  console.log('‚úÖ PASS: OpenAI configured correctly');
} else {
  console.log('‚ùå FAIL: OpenAI configuration incorrect');
  process.exit(1);
}

// Test 4: Gemini (Manual Caching)
console.log('\nüìù Test 4: Google Gemini 2.5 Pro (Manual Caching)');
console.log('-'.repeat(60));

process.env.OPENROUTER_MODEL = 'google/gemini-2.5-pro';
const geminiLamp = new LampCode();
const geminiConfig = geminiLamp.getModelConfig();

console.log('Model:', geminiLamp.model);
console.log('Max Input Tokens:', geminiConfig.maxInputTokens.toLocaleString());
console.log('Supports Caching:', geminiLamp.supportsPromptCaching());
console.log('Auto Caching:', geminiConfig.autoCaching);
console.log('Min Cache Tokens:', geminiConfig.minCacheTokens);
console.log('Requires Manual Cache Control:', geminiLamp.requiresManualCaching());

if (geminiConfig.maxInputTokens === 1000000 && 
    geminiConfig.autoCaching === false && 
    geminiLamp.requiresManualCaching() === true &&
    geminiConfig.minCacheTokens === 2048) {
  console.log('‚úÖ PASS: Gemini configured correctly');
} else {
  console.log('‚ùå FAIL: Gemini configuration incorrect');
  process.exit(1);
}

// Test 5: DeepSeek (Automatic Caching)
console.log('\nüìù Test 5: DeepSeek (Automatic Caching)');
console.log('-'.repeat(60));

process.env.OPENROUTER_MODEL = 'deepseek/deepseek-chat';
const deepseekLamp = new LampCode();
const deepseekConfig = deepseekLamp.getModelConfig();

console.log('Model:', deepseekLamp.model);
console.log('Max Input Tokens:', deepseekConfig.maxInputTokens.toLocaleString());
console.log('Supports Caching:', deepseekLamp.supportsPromptCaching());
console.log('Auto Caching:', deepseekConfig.autoCaching);
console.log('Requires Manual Cache Control:', deepseekLamp.requiresManualCaching());

if (deepseekConfig.maxInputTokens === 64000 && 
    deepseekConfig.autoCaching === true && 
    deepseekLamp.requiresManualCaching() === false) {
  console.log('‚úÖ PASS: DeepSeek configured correctly');
} else {
  console.log('‚ùå FAIL: DeepSeek configuration incorrect');
  process.exit(1);
}

// Test 6: Unknown Model (Default Config)
console.log('\nüìù Test 6: Unknown Model (Default Config)');
console.log('-'.repeat(60));

process.env.OPENROUTER_MODEL = 'unknown/test-model';
const unknownLamp = new LampCode();
const unknownConfig = unknownLamp.getModelConfig();

console.log('Model:', unknownLamp.model);
console.log('Max Input Tokens:', unknownConfig.maxInputTokens.toLocaleString());
console.log('Supports Caching:', unknownLamp.supportsPromptCaching());
console.log('Auto Caching:', unknownConfig.autoCaching || false);
console.log('Requires Manual Cache Control:', unknownLamp.requiresManualCaching());

if (unknownConfig.maxInputTokens === 8000 && 
    unknownLamp.supportsPromptCaching() === false) {
  console.log('‚úÖ PASS: Unknown model uses default config');
} else {
  console.log('‚ùå FAIL: Unknown model configuration incorrect');
  process.exit(1);
}

// Summary
console.log('\n' + '='.repeat(60));
console.log('‚úÖ All Prompt Caching Tests Passed!');
console.log('='.repeat(60));

console.log('\nüìä Summary:');
console.log('  ‚Ä¢ Grok: 256K context, automatic caching');
console.log('  ‚Ä¢ Claude: 180K context, manual caching (4 breakpoints)');
console.log('  ‚Ä¢ OpenAI: 120K context, automatic caching (1024 min)');
console.log('  ‚Ä¢ Gemini: 1M context, manual caching (2048 min)');
console.log('  ‚Ä¢ DeepSeek: 64K context, automatic caching');
console.log('  ‚Ä¢ Unknown: 8K context, no caching');

console.log('\nüí° Implementation Details:');
console.log('  ‚Ä¢ Helper methods working correctly');
console.log('  ‚Ä¢ Model detection functioning');
console.log('  ‚Ä¢ Cache control logic ready');
console.log('  ‚Ä¢ Backward compatibility maintained');

console.log('\nüéâ Prompt caching implementation verified!\n');

process.exit(0);

